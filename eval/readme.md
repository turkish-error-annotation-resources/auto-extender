## Evaluation Directory (`eval/`)

This directory contains the necessary gold-reference data and scripts to evaluate the system's performance against human annotations. It includes manual annotation files, system-generated outputs, and a Python script for calculating various performance metrics.

### 1. Data Files

* **`manual_annotation_annotator1.xlsx`**
* **`manual_annotation_annotator5.xlsx`**
    * **Description:** These files contain the ground-truth facet label annotations performed by two independent human annotators. They are used to measure inter-annotator agreement and serve as the gold standard for system evaluation. Annotator#1 has more expertise than Annotator#5, therefore we used the first file to present evaluation scores in the publication.

* **`system_output.xlsx`**
    * **Description:** This file contains the enriched facet label annotations generated by the system.

### 2. Evaluation Script

* **`eval.py`**
    * **Description:** This script provides three primary methods for evaluating the quality of the annotations and the system's performance:

    1.  **`calculate_combined_kappa`**
        * **Purpose:** Calculates the **Inter-Annotator Agreement (IAA)** between the two human annotators using Cohen's Kappa score.
        * **Input:** `manual_annotation_annotator1.xlsx` and `manual_annotation_annotator2.xlsx`.

    2.  **`eval_facet_wise_accuracy`**
        * **Purpose:** Evaluates the system's performance on a per-facet basis.
        * **Metrics:** Calculates **facet-wise accuracy** (comparing system output vs. human annotation for specific columns) and computes the accuracy for individual facets and **Macro-Average Accuracy** across all facets.

    3.  **`eval_exact_match_accuracy`**
        * **Purpose:** Performs a strict evaluation where the entire set of facets for an entry must match the human annotation perfectly.
        * **Metric:** Calculates **Annotation-wise (Exact-Match) Accuracy**.